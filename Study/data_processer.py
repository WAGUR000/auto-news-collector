import pendulum
from datetime import datetime  # [ìˆ˜ì •] ëª¨ë“ˆ ëŒ€ì‹  í´ë˜ìŠ¤ë¥¼ import í•´ì•¼ strptime ì‚¬ìš© ê°€ëŠ¥
from urllib.parse import urlparse
from news_organization_lists import NEWS_OUTLET_MAP
from extract_keywords import get_keywords  
import html  # íŒŒì´ì¬ ë‚´ì¥ ëª¨ë“ˆ
import pandas as pd
import os
import json
import numpy as np
import csv
from decimal import Decimal
from psycopg2.extras import execute_values

def chunked(iterable, n): 
    """iterableì„ nê°œì”© ë¬¶ì–´ì„œ ë°˜í™˜ (Gemini/Groq API ë°°ì¹˜ ì²˜ë¦¬ìš©)"""
    for i in range(0, len(iterable), n):
        yield iterable[i:i + n]
def clean_text(text): 
    """í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜ (HTML íƒœê·¸ ì œê±° ë° ëª¨ë“  íŠ¹ìˆ˜ë¬¸ì ìë™ ë³€í™˜)"""
    if not isinstance(text, str):
        return ""
    
    # 1. íƒœê·¸ ì œê±° (ê¸°ì¡´ ë°©ì‹ ìœ ì§€í•´ë„ ë¬´ë°©, re ëª¨ë“ˆ ì¶”ì²œ)
    text = text.replace("<b>", "").replace("</b>", "")
    
    # 2. ëª¨ë“  HTML ì—”í‹°í‹°(&quot;, &amp;, &lt; ë“±) í•œ ë°©ì— ë³€í™˜
    text = html.unescape(text) 
    
    return text.strip()

def get_outlet_name(original_link): 
    """
    ì›ë³¸ ë§í¬ì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•˜ì—¬ ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° 'ê¸°íƒ€ì–¸ë¡ ì‚¬'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not original_link:
        return 'ê¸°íƒ€ì–¸ë¡ ì‚¬'
    try:
        domain = urlparse(original_link).netloc
        return NEWS_OUTLET_MAP.get(domain, 'ê¸°íƒ€ì–¸ë¡ ì‚¬')
    except Exception:
        return 'ê¸°íƒ€ì–¸ë¡ ì‚¬'
    
def update_articles_with_topic(original_articles, groq_results):
    # 1. ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•œ Topic ë§µ ìƒì„±
    requested_ids = {a['temp_id'] for a in original_articles if 'temp_id' in a}
    received_ids = {r['temp_id'] for r in groq_results if 'temp_id' in r}
    
    missing_ids = requested_ids - received_ids
    if missing_ids:
        print(f"ğŸš¨ [ì¹˜ëª…ì  ì˜¤ë¥˜] ìš”ì²­í–ˆì§€ë§Œ ì‘ë‹µë°›ì§€ ëª»í•œ temp_id: {missing_ids}")
        # ì—¬ê¸°ì„œ missing_idsì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë“¤ì€ í† í”½ ìƒì„±ì— ì‹¤íŒ¨í•œ ê²ƒì…ë‹ˆë‹¤.
    
    topic_map = {item['temp_id']: item['topic'] for item in groq_results if 'topic' in item and 'temp_id' in item}
    
    # êµ°ì§‘ IDë³„ Topic ë§µ ìƒì„± (ì „íŒŒìš©)
    cluster_topic_map = {}

    # [1ì°¨ ìˆœíšŒ] Topic ë³‘í•©
    for article in original_articles:
        t_id = article.get('temp_id')
        if t_id and t_id in topic_map:
            article['topic'] = topic_map[t_id]
        
        if article.get('topic') and article.get('clusterId') is not None:
             cluster_topic_map[article['clusterId']] = article['topic']

    valid_articles = []
    
    print("--- ğŸ“ ê¸°ì‚¬ ë³‘í•© ë° DB ì €ì¥ìš© ë°ì´í„° ì •ì œ(PK/SK/Outlet/Keyword)ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. ---")

    # [2ì°¨ ìˆœíšŒ] ì •ì œ ë° ê²€ì¦
    for article in original_articles:
        try:
            # 2. Topic ì „íŒŒ
            if not article.get('topic') and article.get('clusterId') in cluster_topic_map:
                article['topic'] = cluster_topic_map[article['clusterId']]

            if not article.get('topic'):
                article['topic'] = article.get('title', '')
                print(f"âš ï¸ Topic ìƒì„± ì‹¤íŒ¨ë¡œ ì œëª© ì‚¬ìš©: {article['title'], article['clusterId'],article['is_representative']}")

            # 3. Outlet ë§¤í•‘
            target_link = article.get('originallink') or article.get('link', '')
            article['outlet'] = get_outlet_name(target_link)

            # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
            try:
                # get_keywordsëŠ” ì´ì œ ['ë‹¨ì–´', 'ë‹¨ì–´']ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
                raw_keywords = get_keywords(article)
            except Exception:
                raw_keywords = []

            # [ì•ˆì „ì¥ì¹˜] ë§Œì•½ ì—¬ì „íˆ ê¼¬ì—¬ìˆë‹¤ë©´ ê°•ì œë¡œ í´ì¤ë‹ˆë‹¤.
            final_keywords = []
            if isinstance(raw_keywords, list):
                for k in raw_keywords:
                    if isinstance(k, str):
                        final_keywords.append(k)
                    elif isinstance(k, dict) and 'S' in k: # {"S": "ê°’"} í˜•íƒœë¼ë©´ ê°’ë§Œ êº¼ëƒ„
                         final_keywords.append(k['S'])
            
            # ì—¬ê¸°ì„œ article['keywords']ëŠ” ë¬´ì¡°ê±´ ['A', 'B', 'C'] í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.
            article['keywords'] = final_keywords

            # 5. ë‚ ì§œ íŒŒì‹±
            pub_date_str = article.get('pubDate', '').strip()
            # ë„¤ì´ë²„ API ì›ë³¸ í•„ë“œê°€ ì—†ìœ¼ë©´ ê°€ê³µëœ í•„ë“œ í™•ì¸
            if not pub_date_str:
                pub_date_str = str(article.get('pub_date', '')).strip()
            
            if not pub_date_str:
                print(f"âš ï¸ ë‚ ì§œ í•„ë“œ ì—†ìŒ (Skip): {article.get('title')}")
                continue 

            try:
                # RFC 822 (Mon, 09 Dec...)
                dt_object = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
            except ValueError:
                try:
                    # ISO 8601 ë“± ê¸°íƒ€ í¬ë§· ì‹œë„
                    dt_object = pendulum.parse(pub_date_str)
                except Exception:
                    print(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ (Skip): {pub_date_str}")
                    continue
            
            p_date = pendulum.instance(dt_object)

            # 6. PK/SK ìƒì„±
            article['pub_date'] = p_date.to_iso8601_string()
            article['PK'] = p_date.to_date_string()
            article['SK'] = f"{p_date.to_iso8601_string()}#{article.get('link', '')}"

            # ë¶ˆí•„ìš” í•„ë“œ ì •ë¦¬
            if 'pubDate' in article: del article['pubDate']
            if 'temp_id' in article: del article['temp_id']

            valid_articles.append(article)

        except Exception as e:
            print(f"âŒ ë°ì´í„° ì •ì œ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e} / ê¸°ì‚¬: {article.get('title')}")
            continue

    print(f"--- âœ… ì²˜ë¦¬ ì™„ë£Œ: {len(valid_articles)}ê±´ ì •ì œë¨ ---")
    return valid_articles

def process_keywords(val):
            # 1. ê°’ì´ ì•„ì˜ˆ ì—†ê±°ë‚˜ Noneì¸ ê²½ìš° (ë‹¨ì¼ ê°’ ì²´í¬ì— ì•ˆì „í•œ ë°©ì‹)
            if val is None or (isinstance(val, float) and np.isnan(val)):
                return json.dumps([], ensure_ascii=False)
            
            # 2. ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” íŠœí”Œì¸ ê²½ìš° (DynamoDBì—ì„œ ê°€ì¥ í”í•œ í˜•íƒœ)
            if isinstance(val, (list, tuple, np.ndarray)):
                # ë¦¬ìŠ¤íŠ¸ ë‚´ë¶€ì— Decimal ê°ì²´ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¼ë°˜ ê°ì²´ë¡œ ë³€í™˜ ì‹œë„
                clean_list = [float(i) if isinstance(i, Decimal) else i for i in val]
                return json.dumps(list(clean_list), ensure_ascii=False)
            
            # 3. ë¬¸ìì—´ í˜•íƒœì¸ ê²½ìš° (CSV/V4 ì†ŒìŠ¤ ë“±)
            if isinstance(val, str):
                val = val.strip()
                if not val or val == "[]":
                    return json.dumps([], ensure_ascii=False)
                try:
                    # ê¸°ì¡´ ë¡œì§: ëŒ€ê´„í˜¸ ì œê±° ë° ë”°ì˜´í‘œ ì •ë¦¬
                    s = val.strip("[]")
                    res = [tok.strip().strip("'").strip('"') for tok in s.split(",") if tok.strip()]
                    return json.dumps(res, ensure_ascii=False)
                except:
                    return json.dumps([], ensure_ascii=False)
            
            return json.dumps([], ensure_ascii=False)


def data_cleaning(articles):
    if not articles:
        print("ìˆ˜ì§‘í•  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        df = pd.DataFrame(articles)
        # 4. ë°ì´í„° ì „ì²˜ë¦¬ (ìˆœì„œ ì¤‘ìš”)
        # DynamoDBì˜ Decimal íƒ€ì…ì„ float/intë¡œ ë³€í™˜ (CSV ì €ì¥ ì‹œ ì—ëŸ¬ ë°©ì§€)
        for col in df.columns:
            if df[col].apply(lambda x: isinstance(x, Decimal)).any():
                df[col] = df[col].apply(lambda x: float(x) if isinstance(x, Decimal) else x)

        # ë¶ˆí•„ìš” ì»¬ëŸ¼ ì œê±° ë° link ì¶”ì¶œ
        df = df.drop(columns=['penalty_applied'], errors='ignore')
        if 'SK' in df.columns:
            df['link'] = df['SK'].str.split('#').str[1]
            # SKëŠ” ë‚˜ì¤‘ì— pkë¡œ ë³€í™˜ë  PKê°€ ìˆìœ¼ë¯€ë¡œ ì‚­ì œ
            df = df.drop(columns=['SK'])

        # 5. í•µì‹¬ ë¡œì§ ì ìš©
        # 
        
        # í‚¤ì›Œë“œ ì²˜ë¦¬ í•¨ìˆ˜

        if 'keywords' in df.columns:
            df['keywords'] = df['keywords'].apply(process_keywords)
        
        # ì„¤ëª… ë° ê¸°íƒ€ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        df['description'] = df['description'].fillna('')
        df['is_representative'] = df['is_representative'].fillna(True).astype(bool)
        # 6. ì»¬ëŸ¼ í‘œì¤€í™” (DB ì ì¬ìš©)
        MASTER_COLUMNS = [
            'pk', 'originallink', 'main_category', 'outlet', 'pub_date',
            'description', 'title', 'is_representative', 'importance', 'clusterid',
            'sub_category', 'topic', 'sentiment', 'keywords', 'link'
        ]

        def finalize_for_db(target_df):
            # ì†Œë¬¸ì ë³€í™˜ (PK -> pk, clusterId -> clusterid ë“±)
            target_df.columns = [col.lower() for col in target_df.columns]
            
            # ëˆ„ë½ëœ ì»¬ëŸ¼ ê¸°ë³¸ê°’ ì±„ìš°ê¸°
            for col in MASTER_COLUMNS:
                if col not in target_df.columns:
                    target_df[col] = ""
            
            # ì¤‘ìš”ë„(importance) ë“± ìˆ«ìí˜• ê¸°ë³¸ê°’ ë³´ì • (í•„ìš”ì‹œ)
            target_df['importance'] = pd.to_numeric(target_df['importance'], errors='coerce').fillna(0).astype(int)
            
            return target_df[MASTER_COLUMNS]

        df = finalize_for_db(df)

        # 7. í•„ìˆ˜ ì‹ë³„ì ì—†ëŠ” í–‰ ìµœì¢… ì œê±°
        df.dropna(subset=['pk', 'link'], inplace=True)
        print(df.info())
        # 8. ì €ì¥
        return df



def bulk_insert_articles(conn, articles_df):
    """
    articles_df: Pandas DataFrame (data_cleaningì˜ ê²°ê³¼ë¬¼)
    """
    print("--- ğŸ“ PostgreSQL ë²Œí¬ ì‚½ì… ì‹œì‘ ---")
    
    # 1. ë§Œì•½ ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ì™”ë‹¤ë©´ ë‹¤ì‹œ DFë¡œ ë³€í™˜í•˜ê±°ë‚˜, 
    # ì—¬ê¸°ì„œëŠ” ë©”ì¸ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•´ì„œ ë„˜ê¸´ë‹¤ê³  ê°€ì •í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if isinstance(articles_df, pd.DataFrame):
        data_list = articles_df.to_dict('records')
    else:
        data_list = articles_df

    cur = conn.cursor()
    
    # 2. íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìˆœì„œ ì£¼ì˜: SQLë¬¸ì˜ ì»¬ëŸ¼ ìˆœì„œì™€ ì¼ì¹˜í•´ì•¼ í•¨)
    # data_cleaningì—ì„œ ì´ë¯¸ lowercase ì²˜ë¦¬ê°€ ë˜ì—ˆìœ¼ë¯€ë¡œ í‚¤ ê°’ì€ ì†Œë¬¸ìì…ë‹ˆë‹¤.
    data_tuples = [
        (
            a.get('pk'), 
            a.get('link'),
            a.get('originallink'),
            a.get('main_category'),
            a.get('outlet'),
            a.get('pub_date'),
            a.get('description'),
            a.get('title'),
            a.get('is_representative', False),
            int(a.get('importance', 0)), # smallint ëŒ€ì‘
            a.get('clusterid'),
            a.get('sub_category'),
            a.get('topic'),
            float(a.get('sentiment', 0.0)),
            a.get('keywords') # data_cleaningì—ì„œ ì´ë¯¸ json.dumps ë¬¸ìì—´ë¡œ ë³€í™˜ë¨
        ) for a in data_list
    ]

    query = """
        INSERT INTO articles_table (
            pk, link, originallink, main_category, outlet, 
            pub_date, description, title, is_representative, 
            importance, clusterid, sub_category, topic, 
            sentiment, keywords
        ) VALUES %s
        ON CONFLICT (pk, link) DO UPDATE SET
            topic = EXCLUDED.topic,
            sentiment = EXCLUDED.sentiment,
            importance = EXCLUDED.importance,
            keywords = EXCLUDED.keywords;
    """

    try:
        execute_values(cur, query, data_tuples)
        conn.commit()
        print(f"âœ… ì´ {len(data_tuples)}ê±´ ì²˜ë¦¬ ì™„ë£Œ (ì¤‘ë³µ ì—…ë°ì´íŠ¸ í¬í•¨)")
    except Exception as e:
        conn.rollback()
        print(f"âŒ ë²Œí¬ ì‚½ì… ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        raise e # ì—ëŸ¬ë¥¼ ë‹¤ì‹œ ë˜ì ¸ì„œ ë¡œê·¸ì— ì°íˆê²Œ í•¨
    finally:
        cur.close()
        