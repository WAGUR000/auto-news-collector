import trafilatura
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed


def fetch_article_content(url):
    """
    URLì—ì„œ ë³¸ë¬¸ê³¼ ëŒ€í‘œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    Returns: (body: str | None, image_url: str | None)
    """
    if not url:
        return None, None
    try:
        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None, None

        body = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=False,
            favor_precision=True,
            no_fallback=False,
        )

        image_url = None
        soup = BeautifulSoup(downloaded, "html.parser")
        og_image = soup.find("meta", property="og:image")
        if og_image:
            image_url = og_image.get("content")

        return body, image_url

    except Exception:
        return None, None


def crawl_articles(articles, max_workers=10):
    """
    ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì˜ originallinkë¥¼ ë³‘ë ¬ í¬ë¡¤ë§í•˜ì—¬ body, image_url í•„ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    - originallinkê°€ ì—†ëŠ” ê¸°ì‚¬(ë„¤ì´ë²„ë‰´ìŠ¤ ì›ë¬¸)ëŠ” body=None, image_url=Noneìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    - í¬ë¡¤ë§ ì‹¤íŒ¨ ì‹œì—ë„ íŒŒì´í”„ë¼ì¸ì´ ì¤‘ë‹¨ë˜ì§€ ì•Šê³  Noneìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    """
    to_crawl = {
        i: article["originallink"]
        for i, article in enumerate(articles)
        if article.get("originallink")
    }
    print(f"--- ğŸŒ ì›ë¬¸ í¬ë¡¤ë§ ì‹œì‘: ì „ì²´ {len(articles)}ê±´ ì¤‘ {len(to_crawl)}ê±´ ëŒ€ìƒ ---")

    results = {i: (None, None) for i in range(len(articles))}

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_idx = {
            executor.submit(fetch_article_content, url): idx
            for idx, url in to_crawl.items()
        }
        for future in as_completed(future_to_idx):
            idx = future_to_idx[future]
            try:
                results[idx] = future.result()
            except Exception:
                results[idx] = (None, None)

    success_count = sum(1 for body, _ in results.values() if body)
    print(f"--- âœ… í¬ë¡¤ë§ ì™„ë£Œ: {success_count}/{len(to_crawl)}ê±´ ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ ---")

    for i, article in enumerate(articles):
        article["body"], article["image_url"] = results[i]

    return articles
