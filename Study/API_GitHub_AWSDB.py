import pendulum
from datetime import datetime
import argparse
from api_handler import naver_api_request, groq_api_request
from dotenv import load_dotenv
from aws_handler import get_recent_articles, save_data
from clustering_news import cluster_news
from data_processer import chunked, update_articles_with_topic, clean_text, data_cleaning, bulk_insert_articles
from predict import NewsClassifier
from extract_keywords import get_keywords
from kiwipiepy import Kiwi
import psycopg2
import os


# ì „ì—­ Kiwi ê°ì²´ (í•¨ìˆ˜ë“¤ì´ ì°¸ì¡°í•¨)
kiwi = Kiwi()

# 1. ì†Œë¶„ë¥˜/ê°ì • ë¶„ì„ìš© í† í¬ë‚˜ì´ì €
def korean_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'VA', 'XR', 'MAG', 'SL']]

# 2. ì¤‘ìš”ë„ ë¶„ì„ìš© í† í¬ë‚˜ì´ì €
def importance_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'XR', 'SN']]

# 3. ëŒ€ë¶„ë¥˜ìš© í´ë˜ìŠ¤ í† í¬ë‚˜ì´ì € (í´ë˜ìŠ¤ë¡œ í•™ìŠµí–ˆë‹¤ë©´ í•„ìš”)
class KiwiTokenizer:
    def __init__(self):
        self.kiwi = Kiwi()
    def __call__(self, text):
        return [t.form for t in self.kiwi.tokenize(text) if t.tag in ['NNG', 'NNP']]
    def __getstate__(self):
        state = self.__dict__.copy()
        if 'kiwi' in state: del state['kiwi']
        return state
    def __setstate__(self, state):
        self.__dict__.update(state)
        self.kiwi = Kiwi()

def main(is_test_mode=False): #is_test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì´ê³  --testë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ ì…ë ¥ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸
    # .\venv\Scripts\activate (CMDìš©, Git Bashë¡œëŠ” ë¶ˆê°€ëŠ¥)
    # python Study/API_GitHub_AWSDB.py --test (í…ŒìŠ¤íŠ¸ í™˜ê²½ ì‹¤í–‰ --test ì˜µì…˜ í•„ìš”)
    load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ. ì—†ì„ê²½ìš° ë„˜ì–´ê°

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° API í˜¸ì¶œëŸ‰ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
    if is_test_mode:
        print("--- ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì‹ ê·œ 2ê°œ + ê¸°ì¡´ 2ê°œ) ---")
        display_count = 30
        batch_size = 10
        recent_articles_limit = 500
    else:
        display_count = 200
        batch_size = 20
        recent_articles_limit = 2000

    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ /  ë§¤ê°œë³€ìˆ˜ : í‘œì‹œí•  ë‰´ìŠ¤ ê°œìˆ˜
    raw_articles = naver_api_request(display_count=display_count)
    classifier = NewsClassifier()
    # 2. LMê¸°ë°˜ ì¤‘ìš”ë„, ê°ì •ë¶„ì„, ëŒ€/ì†Œë¶„ë¥˜
    analyzed_articles = [] # ë¶„ì„ì´ ì™„ë£Œëœ ê¸°ì‚¬ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

    for article in raw_articles:
        # HTML íƒœê·¸ ì •ì œ
        clean_title =clean_text(article.get('title', ''))
        clean_desc = clean_text(article.get('description', ''))

        # ì •ì œëœ í…ìŠ¤íŠ¸ë¡œ ë®ì–´ì“°ê¸°
        article['title'] = clean_title
        article['description'] = clean_desc

        analysis_result = classifier.predict(clean_title, clean_desc)
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ article ë”•ì…”ë„ˆë¦¬ì— ë¶„ì„ í•„ë“œ ì¶”ê°€)
        article.update(analysis_result)
        
        analyzed_articles.append(article)

    # 3. êµ°ì§‘í™”
    print("--- ğŸ’¾ DynamoDBì—ì„œ êµ°ì§‘í™” ë¹„êµë¥¼ ìœ„í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ---")
    recent_db_articles = get_recent_articles(limit=recent_articles_limit)
    print(f"--- {len(recent_db_articles)}ê°œì˜ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ---")
    CLUSTERING_THRESHOLD = 0.77 # êµ°ì§‘í™” ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
    clustered_articles=cluster_news(recent_db_articles, analyzed_articles, threshold=CLUSTERING_THRESHOLD)

    # 4. Groq API ìš”ì²­ì„ ìœ„í•œ ì„ì‹œ ID ë¶€ì—¬ / ë§¤ê°œë³€ìˆ˜ : ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    prompt_targets = [] 
    for i, item in enumerate(clustered_articles):
        # ì´ë¯¸ cluster_newsì—ì„œ ì‹ ê·œ ê¸°ì‚¬ë§Œ í•„í„°ë§ë˜ì–´ ë„˜ì–´ì˜´
        if item.get('is_representative') == 1:
            item['temp_id'] = f"article_{i}"
            prompt_targets.append(item)
    
    print(f"--- ğŸ¤– ìš”ì•½ ë° í† í”½ ìƒì„±ì´ í•„ìš”í•œ ê¸°ì‚¬: {len(prompt_targets)}ê°œ ---")
    
    final_articles_to_save = []

    # 5. Groq API í˜¸ì¶œ ë° ë°ì´í„° í›„ì²˜ë¦¬ í†µí•© ìˆ˜í–‰
    # í•œ ë²ˆì— ì „ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì´ ì•„ë‹ˆë¼ë©´ ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ í˜¸ì¶œ í›„ ê²°ê³¼ ëª¨ìœ¼ê¸°
    
    all_groq_results = []
    if prompt_targets:
        for batch in chunked(prompt_targets, batch_size):
            groq_result = groq_api_request(batch) 
            all_groq_results.extend(groq_result)
            
    # â˜… ì—¬ê¸°ì„œ í•œ ë°©ì— ì²˜ë¦¬ (Topic ë³‘í•©, ì „íŒŒ, Outlet, PK/SK, Keyword ì •ì œ)
    # clustered_articles ì „ì²´(ëŒ€í‘œ ê¸°ì‚¬ + ì¼ë°˜ ê¸°ì‚¬)ë¥¼ ë„˜ê²¨ì•¼ ì „íŒŒê°€ ê°€ëŠ¥í•¨
    final_articles_to_save = update_articles_with_topic(clustered_articles, all_groq_results)
    result=data_cleaning(final_articles_to_save)
    try: 
        conn_postgres=psycopg2.connect(
        host=os.environ.get("DB_HOST"),
        database=os.environ.get("DB_NAME"),
        user=os.environ.get("DB_USER"),
        password=os.environ.get("DB_PASSWORD")
        )
    except Exception as e:
        print(f"âš ï¸ PostgreSQL ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

    bulk_insert_articles(conn_postgres, result)

    # 6. ë°ì´í„° ì €ì¥
    # if final_articles_to_save:
    #     print(f"--- ğŸ’¾ ì´ {len(final_articles_to_save)}ê°œì˜ ìœ íš¨í•œ ê¸°ì‚¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ---")
    #     save_data(final_articles_to_save)
    # else:
    #     print("--- ì €ì¥í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ---")
    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ì—¬ DynamoDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument(
        '--test', 
        action='store_true', 
        help='ìŠ¤í¬ë¦½íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (2ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬)'
    )

    args = parser.parse_args()
    main(is_test_mode=args.test)
