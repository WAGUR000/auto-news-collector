import pendulum
from datetime import datetime
import argparse
from api_handler import naver_api_request, groq_api_request
from dotenv import load_dotenv
from aws_handler import get_recent_articles, save_data
from clustering_news import cluster_news
from data_processer import chunked, update_articles_with_topic
from predict import NewsClassifier
from extract_keywords import get_keywords
from kiwipiepy import Kiwi

# ì „ì—­ Kiwi ê°ì²´ (í•¨ìˆ˜ë“¤ì´ ì°¸ì¡°í•¨)
kiwi = Kiwi()

# 1. ì†Œë¶„ë¥˜/ê°ì • ë¶„ì„ìš© í† í¬ë‚˜ì´ì €
def korean_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'VA', 'XR', 'MAG', 'SL']]

# 2. ì¤‘ìš”ë„ ë¶„ì„ìš© í† í¬ë‚˜ì´ì €
def importance_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'XR', 'SN']]

# 3. ëŒ€ë¶„ë¥˜ìš© í´ë˜ìŠ¤ í† í¬ë‚˜ì´ì € (í´ë˜ìŠ¤ë¡œ í•™ìŠµí–ˆë‹¤ë©´ í•„ìš”)
class KiwiTokenizer:
    def __init__(self):
        self.kiwi = Kiwi()
    def __call__(self, text):
        return [t.form for t in self.kiwi.tokenize(text) if t.tag in ['NNG', 'NNP']]
    def __getstate__(self):
        state = self.__dict__.copy()
        if 'kiwi' in state: del state['kiwi']
        return state
    def __setstate__(self, state):
        self.__dict__.update(state)
        self.kiwi = Kiwi()

def main(is_test_mode=False): #is_test_mode: í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì´ê³  --testë¥¼ í†µí•´ ë§¤ê°œë³€ìˆ˜ ì…ë ¥ì‹œ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸
    # .\venv\Scripts\activate (CMDìš©, Git Bashë¡œëŠ” ë¶ˆê°€ëŠ¥)
    # python Study/API_GitHub_AWSDB.py --test (í…ŒìŠ¤íŠ¸ í™˜ê²½ ì‹¤í–‰ --test ì˜µì…˜ í•„ìš”)
    load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ. ì—†ì„ê²½ìš° ë„˜ì–´ê°

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° API í˜¸ì¶œëŸ‰ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
    if is_test_mode:
        print("--- ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì‹ ê·œ 2ê°œ + ê¸°ì¡´ 2ê°œ) ---")
        display_count = 100
        batch_size = 10
        recent_articles_limit = 500
    else:
        display_count = 200
        batch_size = 20
        recent_articles_limit = 2000

    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ /  ë§¤ê°œë³€ìˆ˜ : í‘œì‹œí•  ë‰´ìŠ¤ ê°œìˆ˜
    raw_articles = naver_api_request(display_count=display_count)
    classifier = NewsClassifier()
    # 2. LMê¸°ë°˜ ì¤‘ìš”ë„, ê°ì •ë¶„ì„, ëŒ€/ì†Œë¶„ë¥˜
    analyzed_articles = [] # ë¶„ì„ì´ ì™„ë£Œëœ ê¸°ì‚¬ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

    for article in raw_articles:
        # HTML íƒœê·¸ ì •ì œ
        clean_title =article.get('title', '')
        clean_desc = article.get('description', '')
        
        # ëª¨ë¸ ì˜ˆì¸¡ ìˆ˜í–‰ (predict.py)
        analysis_result = classifier.predict(clean_title, clean_desc)
        
        # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ article ë”•ì…”ë„ˆë¦¬ì— ë¶„ì„ í•„ë“œ ì¶”ê°€)
        article.update(analysis_result)
        
        # ì •ì œëœ í…ìŠ¤íŠ¸ë¡œ ë®ì–´ì“°ê¸° (ì„ íƒ ì‚¬í•­, Groq ë° DB ì €ì¥ì„ ìœ„í•´ ì¶”ì²œ)
        article['title'] = clean_title
        article['description'] = clean_desc
        
        analyzed_articles.append(article)

    # 3. êµ°ì§‘í™”
    print("--- ğŸ’¾ DynamoDBì—ì„œ êµ°ì§‘í™” ë¹„êµë¥¼ ìœ„í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ---")
    recent_db_articles = get_recent_articles(limit=recent_articles_limit)
    print(f"--- {len(recent_db_articles)}ê°œì˜ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ---")
    CLUSTERING_THRESHOLD = 0.70 # êµ°ì§‘í™” ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)
    clustered_articles=cluster_news(recent_db_articles, analyzed_articles, threshold=CLUSTERING_THRESHOLD)

    # 4. Groq API ìš”ì²­ì„ ìœ„í•œ ì„ì‹œ ID ë¶€ì—¬ / ë§¤ê°œë³€ìˆ˜ : ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    prompt_targets = []  # LLMì— ì‹¤ì œë¡œ ë³´ë‚¼ ê¸°ì‚¬ë“¤ë§Œ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

    for i, item in enumerate(clustered_articles):
        # ì‹ ê·œ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (raw_articlesì— ìˆë˜ ê²ƒì¸ì§€ íŒë³„í•˜ëŠ” ë¡œì§ í•„ìš”, ì—¬ê¸°ì„  is_new í”Œë˜ê·¸ ê°€ì •)
        # ë§Œì•½ cluster_newsê°€ ì‹ ê·œ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•œë‹¤ë©´ is_new ì²´í¬ ë¶ˆí•„ìš”
        # ëŒ€í‘œ ê¸°ì‚¬ì¸ì§€(is_representative == 1)ë§Œ í™•ì¸
        if item.get('is_representative') == 1:
            item['temp_id'] = f"article_{i}"
            prompt_targets.append(item)
    
    print(f"--- ğŸ¤– ìš”ì•½ ë° í† í”½ ìƒì„±ì´ í•„ìš”í•œ ê¸°ì‚¬: {len(prompt_targets)}ê°œ ---")
    
    groq_processed_results = []

    # 5. ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ë©° Groq API í˜¸ì¶œ / ë§¤ê°œë³€ìˆ˜ : ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸, ë°°ì¹˜ í¬ê¸°
    for batch in chunked(prompt_targets, batch_size):
        groq_result = groq_api_request(batch) 
        updated_batch = update_articles_with_topic(batch, groq_result) 
        groq_processed_results.extend(updated_batch)
    # 5-1. topic ìƒì„± ê¸°ì‚¬ì™€ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ë³‘í•©
    # 5-1. Topic ìƒì„± ê¸°ì‚¬ì™€ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ë³‘í•©
    # Groq ì²˜ë¦¬ëœ ê¸°ì‚¬ë“¤ì˜ ê²°ê³¼ë¥¼ ì›ë³¸ ë¦¬ìŠ¤íŠ¸(clustered_articles)ì— ë°˜ì˜
    
    # ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´ temp_idë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    groq_map = {item['temp_id']: item for item in groq_processed_results if 'temp_id' in item}

    final_articles_to_save = []
    
    for item in clustered_articles:
        # 1. Groq ì²˜ë¦¬ê°€ ëœ ê¸°ì‚¬ (ëŒ€í‘œ ê¸°ì‚¬)
        if 'temp_id' in item and item['temp_id'] in groq_map:
            updated_item = groq_map[item['temp_id']]
            del updated_item['temp_id']
            final_articles_to_save.append(updated_item)
        
        # 2. [ì¶”ê°€] Groq ëŒ€ìƒì´ ì•„ë‹ˆì—ˆë˜ ë‚˜ë¨¸ì§€ ì‹ ê·œ ê¸°ì‚¬ë“¤
        # (ì´ë¯¸ cluster_news í•¨ìˆ˜ê°€ ì‹ ê·œ ê¸°ì‚¬ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ ë³„ë„ ì¡°ê±´ ì—†ì´ ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤)
        else:
            # í˜¹ì‹œ temp_idê°€ ë‚¨ì•„ìˆì„ ê²½ìš° ì œê±°
            if 'temp_id' in item:
                del item['temp_id']
            final_articles_to_save.append(item)

    # 6. í‚¤ì›Œë“œ ì¶”ì¶œ

    print("--- ğŸ”‘ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì§„í–‰í•©ë‹ˆë‹¤. ---")
    for article in final_articles_to_save:
        # extract.pyì˜ get_keywords í•¨ìˆ˜ í˜¸ì¶œ
        # row['topic'] í˜¹ì€ row['title']ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì¶œí•¨
        article['keywords'] = get_keywords(article)

    # 7. DynamoDB ì €ì¥ì„ ìœ„í•œ PK/SK ìƒì„± ë° ë°ì´í„° ì •ì œ
    print("--- ğŸ“ DynamoDB ì €ì¥ì„ ìœ„í•œ PK/SK ìƒì„± ë° ë°ì´í„° ì •ì œë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ---")
    valid_articles_to_save = []

    for article in final_articles_to_save:
        try:
            # 1. ì›ë³¸ pubDate ë¬¸ìì—´ í™•ì¸ (ë„¤ì´ë²„ API í•„ë“œëª…: pubDate)
            pub_date_str = article.get('pubDate', '').strip()
        
            if not pub_date_str:
                # pubDateê°€ ì—†ìœ¼ë©´ pub_date(snake_case)ê°€ ìˆëŠ”ì§€ í•œ ë²ˆ ë” í™•ì¸ (í˜¹ì‹œ ëª¨ë¥´ë‹ˆ)
                pub_date_str = str(article.get('pub_date', '')).strip()
            
            if not pub_date_str:
                raise ValueError("pubDate ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

            # 2. ë‚ ì§œ íŒŒì‹± (ì‚¬ìš©ìë‹˜ì´ ì‘ì„±í•˜ì‹  ì •í™•í•œ í¬ë§· ì‚¬ìš©)
            # ì˜ˆ: "Tue, 09 Dec 2025 11:23:58 +0900"
            try:
                # ë„¤ì´ë²„ ì›ë³¸ í¬ë§· ì‹œë„
                dt_object = datetime.strptime(pub_date_str, "%a, %d %b %Y %H:%M:%S %z")
            except ValueError:
                # í˜¹ì‹œë¼ë„ í˜•ì‹ì´ ë‹¤ë¥´ê±°ë‚˜ ì´ë¯¸ ISO í¬ë§·ì¸ ê²½ìš° Pendulumìœ¼ë¡œ ìë™ íŒŒì‹± ì‹œë„
                dt_object = pendulum.parse(pub_date_str)

            # 3. Pendulum ê°ì²´ ë³€í™˜
            p_date = pendulum.instance(dt_object)

            # 4. í•„ë“œ ìƒì„±
            # pub_date: ì‹œ:ë¶„:ì´ˆê°€ ëª¨ë‘ í¬í•¨ëœ ISO 8601 ë¬¸ìì—´ (ì˜ˆ: 2025-12-09T11:23:58+09:00)
            article['pub_date'] = p_date.to_iso8601_string()
        
            # PK: ë‚ ì§œë§Œ (ì˜ˆ: 2025-12-09)
            article['PK'] = p_date.to_date_string()
        
            # SK: ì‹œê°„+ë§í¬ (ìœ ë‹ˆí¬ í‚¤)
            article['SK'] = f"{p_date.to_iso8601_string()}#{article.get('link', '')}"

            # 5. ë¶ˆí•„ìš”í•œ ì›ë³¸ ì‚­ì œ
            if 'pubDate' in article:
                del article['pubDate']

            # 6. ìœ íš¨ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            valid_articles_to_save.append(article)

        except Exception as e:
            print(f"âš ï¸ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ (ê±´ë„ˆëœ€): {e}")
            print(f"   - ë¬¸ì œì˜ ë°ì´í„°: {article.get('title', 'ì œëª©ì—†ìŒ')}")

    # 8. ë°ì´í„° ì €ì¥ (ìœ íš¨í•œ ê¸°ì‚¬ë§Œ)
    if valid_articles_to_save:
        print(f"--- ğŸ’¾ ì´ {len(valid_articles_to_save)}ê°œì˜ ìœ íš¨í•œ ê¸°ì‚¬ë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ---")
        save_data(valid_articles_to_save)
    else:
        print("--- ì €ì¥í•  ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ì—¬ DynamoDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument(
        '--test', 
        action='store_true', 
        help='ìŠ¤í¬ë¦½íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (2ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬)'
    )
    args = parser.parse_args()
    main(is_test_mode=args.test)
