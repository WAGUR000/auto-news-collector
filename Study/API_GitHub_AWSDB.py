import boto3
import os
import requests
import pendulum
import google.generativeai as genai
from urllib.parse import quote, urlparse
import json
import argparse
import uuid
from decimal import Decimal
from dotenv import load_dotenv
from boto3.dynamodb.conditions import Key
from sentence_transformers import SentenceTransformer, util
import torch
from news_organization_lists import NEWS_OUTLET_MAP

# --- ì„¤ì •ê°’ ---
DYNAMODB_TABLE_NAME = 'News_Data_v1'
GEMINI_MODEL_NAME = 'gemini-2.5-flash'
AWS_REGION = 'ap-northeast-2'
CLUSTERING_THRESHOLD = 0.85 # êµ°ì§‘í™” ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)

# GitHub Actions í™˜ê²½ì—ì„œëŠ” í‚¤ë¥¼ ì§ì ‘ ë„£ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ ì¸ì¦ë©ë‹ˆë‹¤.
dynamodb = boto3.resource('dynamodb', region_name=AWS_REGION)
table = dynamodb.Table(DYNAMODB_TABLE_NAME)

# SentenceTransformer ëª¨ë¸ ë¡œë“œ (ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ)
# GPUê°€ ìˆìœ¼ë©´ 'cuda', ì—†ìœ¼ë©´ 'cpu'ë¥¼ ìë™ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
device = 'cuda' if torch.cuda.is_available() else 'cpu'
sbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=device)

def save_data(articles_list):
    """DynamoDBì˜ BatchWriterë¥¼ ì‚¬ìš©í•´ ì—¬ëŸ¬ í•­ëª©ì„ í•œë²ˆì— íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with table.batch_writer() as batch:
            for item in articles_list:
                # DynamoDBëŠ” float íƒ€ì…ì„ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Decimalë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                # json.dumpsì™€ loadsë¥¼ ì´ìš©í•˜ë©´ ì¤‘ì²©ëœ êµ¬ì¡°ì˜ floatë„ ëª¨ë‘ Decimalë¡œ ì‰½ê²Œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                item_decimal = json.loads(json.dumps(item), parse_float=Decimal)
                batch.put_item(Item=item_decimal)
        print(f"{len(articles_list)}ê°œì˜ ë°ì´í„° ì €ì¥ ì„±ê³µ.")
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {e}")

def chunked(iterable, n):
    """iterableì„ nê°œì”© ë¬¶ì–´ì„œ ë°˜í™˜"""
    for i in range(0, len(iterable), n):
        yield iterable[i:i + n]

def get_outlet_name(original_link):
    """
    ì›ë³¸ ë§í¬ì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•˜ì—¬ ì–¸ë¡ ì‚¬ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° 'ê¸°íƒ€ì–¸ë¡ ì‚¬'ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not original_link:
        return 'ê¸°íƒ€ì–¸ë¡ ì‚¬'
    try:
        domain = urlparse(original_link).netloc
        return NEWS_OUTLET_MAP.get(domain, 'ê¸°íƒ€ì–¸ë¡ ì‚¬')
    except Exception:
        return 'ê¸°íƒ€ì–¸ë¡ ì‚¬'

def get_recent_articles(limit=100):
    """DynamoDBì—ì„œ ìµœì‹  ê¸°ì‚¬ 'limit'ê°œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        today_str = pendulum.now('Asia/Seoul').to_date_string()
        yesterday_str = pendulum.now('Asia/Seoul').subtract(days=1).to_date_string()

        # ì˜¤ëŠ˜ ê¸°ì‚¬ ì¡°íšŒ (ìµœì‹ ìˆœ)
        response_today = table.query(
            KeyConditionExpression=Key('PK').eq(today_str),
            ScanIndexForward=False
        )
        items = response_today.get('Items', [])

        # ì˜¤ëŠ˜ ê¸°ì‚¬ê°€ ë¶€ì¡±í•˜ë©´ ì–´ì œ ê¸°ì‚¬ë„ ì¡°íšŒ
        if len(items) < limit:
            response_yesterday = table.query(
                KeyConditionExpression=Key('PK').eq(yesterday_str),
                ScanIndexForward=False
            )
            items.extend(response_yesterday.get('Items', []))

        # ëª¨ë“  ê¸°ì‚¬ë¥¼ SK(ì‹œê°„) ê¸°ì¤€ìœ¼ë¡œ ìµœì‹ ìˆœ ì •ë ¬ í›„ limitë§Œí¼ ë°˜í™˜
        items.sort(key=lambda x: x.get('SK', ''), reverse=True)
        return items[:limit]
    except Exception as e:
        print(f"DynamoDBì—ì„œ ìµœê·¼ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        return []

def main(is_test_mode=False):
    """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ë¶„ì„í•˜ê³  DynamoDBì— ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    # ë¡œì»¬ í™˜ê²½ì˜ .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    # GitHub Actions í™˜ê²½ì—ì„œëŠ” .env íŒŒì¼ì´ ì—†ìœ¼ë¯€ë¡œ ì´ ì½”ë“œëŠ” ë¬´ì‹œë©ë‹ˆë‹¤.
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸
    # .\venv\Scripts\activate (CMD, Git Bashë¡œëŠ” ë¶ˆê°€ëŠ¥)
    # python Study/API_GitHub_AWSDB.py --test (í…ŒìŠ¤íŠ¸ í™˜ê²½ ì‹¤í–‰)


    load_dotenv() # .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ. ì—†ì„ê²½ìš° ë„˜ì–´ê°

    GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
    NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
    NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")

    if not all([GEMINI_API_KEY, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET]):
        print("ì—ëŸ¬: í•„ìš”í•œ í™˜ê²½ë³€ìˆ˜(GEMINI_API_KEY, NAVER_CLIENT_ID, NAVER_CLIENT_SECRET)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit(1)

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_MODEL_NAME)

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ê²½ìš° API í˜¸ì¶œëŸ‰ê³¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì…ë‹ˆë‹¤.
    if is_test_mode:
        print("--- ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (ì‹ ê·œ 2ê°œ + ê¸°ì¡´ 2ê°œ) ---")
        display_count = 2
        batch_size = 2
        recent_articles_limit = 2
    else:
        display_count = 100
        batch_size = 10
        recent_articles_limit = 100

    # 1. ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ
    keyword = "ë‰´ìŠ¤"
    enc_keyword = quote(keyword)
    url = f"https://openapi.naver.com/v1/search/news.json?query={enc_keyword}&display={display_count}&start=1&sort=date"
    headers = {"X-Naver-Client-Id": NAVER_CLIENT_ID, "X-Naver-Client-Secret": NAVER_CLIENT_SECRET}

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        news_data = response.json()
        raw_articles = news_data.get("items", [])
    except requests.exceptions.RequestException as e:
        print(f"ë„¤ì´ë²„ ë‰´ìŠ¤ API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        exit(1)

    # 2. ì„ì‹œ ë¼ë²¨ ì¶”ê°€ ë° ë°ì´í„° ì¤€ë¹„
    labeled_articles = []
    for i, item in enumerate(raw_articles):
        # ê° ê¸°ì‚¬ì— ê³ ìœ í•œ IDë¥¼ ë¶€ì—¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— í¬í•¨ì‹œí‚µë‹ˆë‹¤.
        item['temp_id'] = f"article_{i}"
        labeled_articles.append(item)

    processed_articles_for_db = []

    # 3. ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬í•˜ë©° Gemini API í˜¸ì¶œ
    for batch in chunked(labeled_articles, batch_size):
        # APIì— ë³´ë‚¼ ê¸°ì‚¬ ëª©ë¡ì„ ê°„ê²°í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
        articles_for_prompt = [
            {
                "temp_id": item['temp_id'],
                "title": item.get("title", "").replace("<b>", "").replace("</b>", ""),
                "description": item.get("description", "")
            }
            for item in batch
        ]
        
        prompt = (
               f"""ì•„ë˜ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ì…ë‹ˆë‹¤. ê° ê¸°ì‚¬ì— ëŒ€í•´ `temp_id`, ì£¼ìš” í† í”½, ê°ì •, ëŒ€ë¶„ë¥˜, ì†Œë¶„ë¥˜, ì¤‘ìš”ë„ë¥¼ ì¶”ì¶œí•´ JSON ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
                 ë°˜ë“œì‹œ ì…ë ¥ëœ `temp_id`ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•˜ê³ , ì•„ë˜ì˜ ì¶œë ¥ í˜•ì‹ê³¼ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
                  ### í•„ìˆ˜ ê·œì¹™: ëŒ€ë¶„ë¥˜ ì„ íƒ
                   ì•„ë˜ì— ì œì‹œëœ 8ê°œì˜ `ëŒ€ë¶„ë¥˜` ì¤‘ ê°€ì¥ ì í•©í•œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ `category1` ê°’ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.
                    - ì •ì¹˜
                    - ê²½ì œ
                    - ì‚¬íšŒ
                    - IT/ê³¼í•™
                    - ë¬¸í™”/ìƒí™œ
                    - ì—°ì˜ˆ
                    - ìŠ¤í¬ì¸ 
                    - êµ­ì œ
            ì…ë ¥:
            {json.dumps(articles_for_prompt, ensure_ascii=False, indent=2)}
            
            ì¶œë ¥ í˜•ì‹:
            [
              {{
                "temp_id": "article_0",
                "topic": "ì£¼ìš” í† í”½",
                "sentiment": 0.0(ë¶€ì •)~5.0(ì¤‘ë¦½)~10.0(ê¸ì •) ì‚¬ì´ì˜ ì‹¤ìˆ˜ (floatí˜•ì‹, ì†Œìˆ˜ì  ì²«ì§¸ìë¦¬ê¹Œì§€)",
                "category1": "ëŒ€ë¶„ë¥˜",
                "category2": "ì†Œë¶„ë¥˜",
                "importance": 1~10 ì‚¬ì´ì˜ ì •ìˆ˜ (1: ë§¤ìš° ë‚®ìŒ, 10: ë§¤ìš° ë†’ìŒ, intí˜•ì‹)
              }},
              ...
            ]
            """
        )
        
        try:
            response = model.generate_content(prompt)
            json_str = response.text.strip('`').strip('json').strip()
            
            try:
                # ì‘ë‹µì—ì„œ JSON íŒŒì‹±
                gemini_result = json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"Gemini ì‘ë‹µ JSON íŒŒì‹± ì—ëŸ¬: {e}")
                print(f"ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸: {json_str}")
                continue # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ë°°ì¹˜ë¡œ ë„˜ì–´ê°
            # 4. Gemini API ì‘ë‹µê³¼ ì›ë³¸ ë°ì´í„° ê²°í•©
            gemini_map = {item['temp_id']: item for item in gemini_result}
            
            for item in batch:
                temp_id = item['temp_id']
                if temp_id in gemini_map:
                    gemini_info = gemini_map[temp_id]

                    # ì¤‘ìš”ë„ë¥¼ ì•ˆì „í•˜ê²Œ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    importance_val = gemini_info.get("importance")
                    try:
                        # ë¬¸ìì—´ë¡œ ëœ ìˆ«ì('7')ë„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ int()ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                        importance = int(importance_val)
                    except (ValueError, TypeError):
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ(ì˜ˆ: 'ë†’ìŒ', None) ê¸°ë³¸ê°’ 5ë¥¼ ì‚¬ìš©í•˜ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
                        print(f"Warning: 'importance' ê°’ '{importance_val}'ì„(ë¥¼) ì •ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ 5ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (temp_id: {temp_id})")
                        importance = 5

                    # ê°ì •ì„ ì•ˆì „í•˜ê²Œ ì‹¤ìˆ˜í˜•(float)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                    sentiment_val = gemini_info.get("sentiment")
                    try:
                        # ë¬¸ìì—´ë¡œ ëœ ìˆ«ì('7.5')ë‚˜ ì •ìˆ˜(5)ë„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ float()ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                        sentiment = float(sentiment_val)
                    except (ValueError, TypeError):
                        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ(ì˜ˆ: 'ì¤‘ë¦½', None) ê¸°ë³¸ê°’ 5.0ì„ ì‚¬ìš©í•˜ê³  ë¡œê·¸ë¥¼ ë‚¨ê¹ë‹ˆë‹¤.
                        print(f"Warning: 'sentiment' ê°’ '{sentiment_val}'ì„(ë¥¼) ì‹¤ìˆ˜ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ 5.0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (temp_id: {temp_id})")
                        sentiment = 5.0
                    
                    pub_date_obj = pendulum.from_format(item.get("pubDate"), 'ddd, DD MMM YYYY HH:mm:ss ZZ', tz='Asia/Seoul')
                    
        
                    pub_date_str1 = pub_date_obj.format('YYYY-MM-DD')
                    link = item.get("link")

                    original_link = item.get("originallink")
                    partition_key = pub_date_str1
                    sort_key = f"{pub_date_obj.to_iso8601_string()}#{link}"




                    # ìµœì¢… DynamoDB ì €ì¥ìš© ë°ì´í„° ìƒì„±
                    processed_articles_for_db.append({
                        "PK": partition_key, # íŒŒí‹°ì…˜ í‚¤. ë‚ ì§œ(YYYY-MM-DD)
                        "SK": sort_key, # ì •ë ¬ í‚¤. ISO 8601 í˜•ì‹ì˜ ë‚ ì§œ + ë§í¬ (ìœ ì¼ì„± ë³´ì¥)
                        "title": item.get("title", "").replace("<b>", "").replace("</b>", "").replace("&quot;", "\""),
                        "topic": gemini_info.get("topic"),
                        "importance": importance,
                        "sentiment": sentiment,
                        "main_category": gemini_info.get("category1"),
                        "sub_category": gemini_info.get("category2"),
                        "description": item.get("description", "").replace("<b>", "").replace("</b>", "").replace("&quot;", "\""),
                        "pub_date": pub_date_obj.to_iso8601_string(),
                        "originallink": original_link, # ë„¤ì´ë²„ ë‰´ìŠ¤ë§í¬ê°€ ì•„ë‹Œ, ë‰´ìŠ¤ ì œê³µì²˜ì˜ ì›ë³¸ ë§í¬
                        "outlet": get_outlet_name(original_link) # originallink ê¸°ë°˜ìœ¼ë¡œ ì–¸ë¡ ì‚¬ ë¶„ë¥˜
                    })

        except Exception as e:
            print(f"Gemini API í˜¸ì¶œ ë˜ëŠ” ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            continue # ë‹¤ìŒ ë°°ì¹˜ë¡œ ì´ë™

    # 5. DBì—ì„œ ìµœì‹  ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸° (êµ°ì§‘í™” ë¹„êµ ëŒ€ìƒ)
    print("--- ğŸ’¾ DynamoDBì—ì„œ êµ°ì§‘í™” ë¹„êµë¥¼ ìœ„í•œ ìµœì‹  ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ---")
    recent_db_articles = get_recent_articles(limit=recent_articles_limit)
    print(f"--- {len(recent_db_articles)}ê°œì˜ ê¸°ì¡´ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. ---")

    # 5. ë‰´ìŠ¤ êµ°ì§‘í™” (Clustering)
    # ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ì™€ DBì˜ ìµœì‹  ê¸°ì‚¬ë¥¼ í•©ì³ì„œ êµ°ì§‘í™” ìˆ˜í–‰
    all_articles_for_clustering = recent_db_articles + processed_articles_for_db

    if all_articles_for_clustering:
        print(f"--- ğŸ“° ë‰´ìŠ¤ êµ°ì§‘í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (ì´ {len(all_articles_for_clustering)}ê°œ ê¸°ì‚¬, ì„ê³„ê°’: {CLUSTERING_THRESHOLD}) ---")

        # ê° ê¸°ì‚¬ì˜ ì œëª©ê³¼ ì„¤ëª…ì„ í•©ì³ì„œ ë²¡í„°ë¡œ ë³€í™˜í•  ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        corpus = [f"{article['title']}. {article.get('description', '')}" for article in all_articles_for_clustering]

        # SBERT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì¥ë“¤ì„ ì„ë² ë”©(ë²¡í„°í™”)
        embeddings = sbert_model.encode(corpus, convert_to_tensor=True, show_progress_bar=False)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì„ê³„ê°’ ì´ìƒì˜ ìœ ì‚¬ë„ë¥¼ ê°€ì§„ ê¸°ì‚¬ êµ°ì§‘ íƒìƒ‰
        clusters = util.community_detection(embeddings, min_community_size=1, threshold=CLUSTERING_THRESHOLD)

        cluster_id_map = {}
        representative_map = {} # ëŒ€í‘œ ê¸°ì‚¬ ì¸ë±ìŠ¤ë¥¼ ê¸°ë¡í•˜ê¸° ìœ„í•œ ë§µ

        # ê° êµ°ì§‘ì— ëŒ€í•´ ê³ ìœ  ID ë¶€ì—¬ ë° ëŒ€í‘œ ê¸°ì‚¬ ì„¤ì •
        for cluster in clusters:
            # êµ°ì§‘ì˜ ëŒ€í‘œ IDë¡œ ì§§ê³  ê³ ìœ í•œ UUIDë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            cluster_id = uuid.uuid4().hex

            # êµ°ì§‘ì˜ ì²« ë²ˆì§¸ ê¸°ì‚¬ë¥¼ ëŒ€í‘œë¡œ ì§€ì •í•©ë‹ˆë‹¤.
            representative_idx = cluster[0]
            representative_map[representative_idx] = 1

            for article_idx in cluster:
                # ë§µì— 'ê¸°ì‚¬ ì¸ë±ìŠ¤' -> 'UUID clusterId' ì €ì¥
                cluster_id_map[article_idx] = cluster_id

        # ìƒˆë¡œ ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ì„œë§Œ cluster_idì™€ is_representativeë¥¼ í• ë‹¹
        start_index_for_new_articles = len(recent_db_articles)
        for i, article in enumerate(processed_articles_for_db):
            # ì „ì²´ ëª©ë¡ì—ì„œì˜ ì¸ë±ìŠ¤ ê³„ì‚°
            combined_list_index = start_index_for_new_articles + i

            # í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ê¸°ì‚¬ê°€ ì†í•œ êµ°ì§‘ì˜ IDë¥¼ ê°€ì ¸ì˜´
            cluster_id = cluster_id_map.get(combined_list_index)

            if cluster_id:
                article['clusterId'] = cluster_id
                # ëŒ€í‘œ ê¸°ì‚¬ ë§µì„ í™•ì¸í•˜ì—¬ ëŒ€í‘œ ì—¬ë¶€ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
                article['is_representative'] = 1 if representative_map.get(combined_list_index) else 0
            else:
                # êµ°ì§‘ì— ì†í•˜ì§€ ì•Šì€ ê²½ìš°, ìê¸° ìì‹ ì„ ëŒ€í‘œë¡œ ì„¤ì •í•˜ê³  ìƒˆë¡œìš´ UUIDë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.
                article['clusterId'] = uuid.uuid4().hex
                article['is_representative'] = 1
        print(f"--- êµ°ì§‘í™” ì™„ë£Œ. ì´ {len(clusters)}ê°œì˜ êµ°ì§‘ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤. ---")

    # 6. ìµœì¢… ë°ì´í„° ì €ì¥
    if processed_articles_for_db:
        save_data(processed_articles_for_db)
    else:
        print("ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•˜ì—¬ DynamoDBì— ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument(
        '--test', 
        action='store_true', 
        help='ìŠ¤í¬ë¦½íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. (2ê°œ ê¸°ì‚¬ë§Œ ì²˜ë¦¬)'
    )
    args = parser.parse_args()
    main(is_test_mode=args.test)