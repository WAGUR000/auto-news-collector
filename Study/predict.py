import os
import joblib
import numpy as np
import traceback
from kiwipiepy import Kiwi
import torch
from transformers import T5ForConditionalGeneration, AutoTokenizer
from pathlib import Path

# =========================================================
# 0. ê³µí†µ Kiwi ê°ì²´ ìƒì„± (ì „ì—­)
# =========================================================
kiwi = Kiwi()

# =========================================================
# 1. ëª¨ë¸ ë¡œë“œìš© í•¨ìˆ˜/í´ë˜ìŠ¤ ì •ì˜ (í•™ìŠµ ë•Œì™€ ë™ì¼í•´ì•¼ í•¨)
# =========================================================

def korean_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'VA', 'XR', 'MAG', 'SL']]

def importance_tokenizer(text):
    return [t.form for t in kiwi.tokenize(text) if t.tag in ['NNG', 'NNP', 'XR', 'SN']]

class KiwiTokenizer:
    def __init__(self):
        self.kiwi = Kiwi()
    
    def __call__(self, text):
        return [t.form for t in self.kiwi.tokenize(text) if t.tag in ['NNG', 'NNP']]

    def __getstate__(self):
        state = self.__dict__.copy()
        if 'kiwi' in state: del state['kiwi']
        return state

    def __setstate__(self, state):
        self.__dict__.update(state)
        self.kiwi = Kiwi()

# =========================================================
# 2. ë‰´ìŠ¤ ë¶„ì„ê¸° í´ë˜ìŠ¤
# =========================================================
class NewsClassifier:
    def __init__(self, model_dir=None):
        if model_dir is None:
            current_file_path = os.path.abspath(__file__)
            current_dir = os.path.dirname(current_file_path)
            self.model_dir = os.path.join(current_dir, 'models')
        else:
            self.model_dir = model_dir

        print(f"Loading models from: {self.model_dir}")
        self.load_models()

    def load_models(self):
        try:
            self.model_main = joblib.load(os.path.join(self.model_dir, 'main_category_model.pkl'))
            self.model_sub = joblib.load(os.path.join(self.model_dir, 'sub_category_model.pkl'))
            self.model_imp = joblib.load(os.path.join(self.model_dir, 'importance_model.pkl'))
            self.model_sent = joblib.load(os.path.join(self.model_dir, 'sentiment_model.pkl'))
            print("âœ… All models loaded successfully!")
        except Exception as e:
            print(f"âŒ Error loading models: {e}")
            self.model_main = None

    def calculate_penalty(self, text):
        """ì¤‘ìš”ë„ ê±°í’ˆ ì œê±°ë¥¼ ìœ„í•œ ê°•ë ¥í•œ í˜ë„í‹° ê³„ì‚° ë¡œì§ (ì—…ë°ì´íŠ¸ë¨)"""
        penalty = 0
        text_str = str(text).strip()
        
        # 1. ê¸¸ì´ ê¸°ë°˜ í˜ë„í‹°
        if len(text_str) < 10: 
            penalty += 2.0
        elif len(text_str) < 15:
            penalty += 1.0
            
        # 2. ê°ì„±/ì—ì„¸ì´/ê°€ì‹­ì„± í‚¤ì›Œë“œ (ê¸°ì¡´ ë¦¬ìŠ¤íŠ¸ ìœ ì§€ + 'ì§ìº ' ë“±)
        low_quality_keywords = [
            'ì¡¸ì—…', 'ì¶•í•˜', 'ê½ƒê¸¸', 'ì•„ë“¤', 'ë”¸', 'ê°€ì¡±', 'ê·¼í™©', 'í¬ì°©', 
            'ì¼ìƒ', 'ì—¬í–‰', 'ë§›ì§‘', 'ë¨¹ë°©', 'ìœ íŠœë¸Œ', 'ì¸ìŠ¤íƒ€', 'í™”ì œ', 
            'ì¶©ê²©', 'ë…¼ë€', 'ê²½ì•…', 'ê²°êµ­', 'ì•Œê³ ë³´ë‹ˆ', 'ëˆˆê¸¸', 'ê³µê°œ',
            'í‹°ì €', 'í¬ìŠ¤í„°', 'ë¹„í•˜ì¸ë“œ', 'ìŠ¤í‹¸', 'ì˜ˆê³ ', 'ì„ ê³µê°œ',
            'í™”ë³´', 'í¬í† ', 'ì§ìº ', 'íŒ¨ì…˜', 'ë£©', 'ì½”ë””', 'ë·°í‹°',
            'íŒŒì´íŒ…', 'ë¬¼ì˜¤ë¥¸', 'ì—¬ì‹ ', 'ë‚¨ì‹ ', 'ìíƒœ' # (ì¶”ê°€ë¨)
        ]
        
        # 3. [ì‹ ê·œ] ì—°ì˜ˆ/ë“œë¼ë§ˆ/í™ë³´ íŠ¹í™” í‚¤ì›Œë“œ (ê°€ì¥ ì¤‘ìš”!)
        # ì´ ë‹¨ì–´ë“¤ì´ ìˆìœ¼ë©´ 'ì‹¬ê°í•œ ë‹¨ì–´(ë³µìˆ˜, ì‚´ì¸)'ê°€ ìˆì–´ë„ í”½ì…˜ì„ì„ ì¸ì§€í•˜ê³  ê°ì 
        entertainment_keywords = [
            'ë“œë¼ë§ˆ', 'ì˜ˆëŠ¥', 'ë°©ì†¡', 'ì²«ë°©', 'ë³¸ë°©', 'ì‚¬ìˆ˜', 'ì¤„ê±°ë¦¬', 
            'ê´€ì „í¬ì¸íŠ¸', 'ë“±ì¥ì¸ë¬¼', 'ì¸ë¬¼ê´€ê³„ë„', 'ì‹œì²­ë¥ ', 'OST',
            'ë°°ìš°', 'ê°€ìˆ˜', 'ì•„ì´ëŒ', 'ì»´ë°±', 'ë°ë·”', 'ì†Œì†ì‚¬', 'ì „ì†ê³„ì•½',
            'ì œì‘ë°œí‘œíšŒ', 'ì‡¼ì¼€ì´ìŠ¤', 'ë¬´ëŒ€ì¸ì‚¬', 'ì‹œì‚¬íšŒ', 'ë ˆë“œì¹´í«',
            'ì¼ì¼ë“œë¼ë§ˆ', 'ì£¼ë§ë“œë¼ë§ˆ', 'ìˆ˜ëª©ë“œë¼ë§ˆ', 'ì›”í™”ë“œë¼ë§ˆ'
        ]

        # 4. ë‹¨ìˆœ í–‰ì‚¬/ì•Œë¦¼ í‚¤ì›Œë“œ
        event_keywords = [
            'ê°œìµœ', 'ì„±ë£Œ', 'ì§„í–‰', 'ì°¸ì„', 'ëª¨ì§‘', 'ì„ ì •', 'ìˆ˜ìƒ', 'í‘œì°½', 
            'ê¸°íƒ', 'ì „ë‹¬', 'ì²´ê²°', 'MOU', 'í˜‘ì•½', 'ì´ë²¤íŠ¸', 'í”„ë¡œëª¨ì…˜', 
            'í• ì¸', 'íŠ¹ê°€', 'ì¶œì‹œ', 'ì˜¤í”ˆ', 'ê¸°ë…', 'ì†Œì‹', 'ê²Œì‹œíŒ', 'ì¸ì‚¬', 'ë¶€ê³ '
        ]
        
        # --- í‚¤ì›Œë“œ ê²€ì‚¬ ë° ì ìˆ˜ ëˆ„ì  ---
        
        # ê°€ì‹­ì„± í‚¤ì›Œë“œ (ê±´ë‹¹ 1.5ì , ìµœëŒ€ 3ì )
        hit_low = 0
        for k in low_quality_keywords:
            if k in text_str: hit_low += 1.5
        penalty += min(hit_low, 3.0)
            
        # [í•µì‹¬] ì—°ì˜ˆ/ë“œë¼ë§ˆ í‚¤ì›Œë“œ (ê±´ë‹¹ 2.0ì , ê°•ë ¥ ì œì¬)
        hit_ent = 0
        for k in entertainment_keywords:
            if k in text_str: hit_ent += 2.0
        penalty += min(hit_ent, 4.0)

        # í–‰ì‚¬ í‚¤ì›Œë“œ (ê±´ë‹¹ 1.0ì )
        for k in event_keywords:
            if k in text_str: penalty += 1.0

        # 5. íŠ¹ìˆ˜ë¬¸ì ê³¼ë‹¤ ì‚¬ìš© (ê´‘ê³ /ì–´ê·¸ë¡œì„±)
        special_chars = text_str.count('!') + text_str.count('?') + text_str.count('~')
        if special_chars >= 2: penalty += 1.0

        # ìµœëŒ€ 7ì ê¹Œì§€ë§Œ ê°ì  (0ì  ë°©ì§€ìš© min ì²˜ë¦¬ëŠ” predictì—ì„œ í•¨)
        return min(penalty, 7.0)

    def predict(self, title, description):
        if self.model_main is None:
            return {"error": "Models not loaded"}

        # 1. ì „ì²˜ë¦¬
        title = str(title) if title else ""
        desc = str(description) if description else ""
        input_text = title + " " + desc
        input_list = [input_text]

        # 2. ì˜ˆì¸¡ ìˆ˜í–‰
        try:
            # Main Category
            main_cat = self.model_main.predict(input_list)[0]
            
            # Sub Category
            if hasattr(self.model_sub, "predict_proba"):
                proba = self.model_sub.predict_proba(input_list)[0]
                max_prob = np.max(proba)
                if max_prob < 0.4:
                    sub_cat = "ê¸°íƒ€"
                else:
                    sub_cat = self.model_sub.classes_[np.argmax(proba)]
            else:
                sub_cat = self.model_sub.predict(input_list)[0]

            # Importance (0~10) - í˜ë„í‹° ì ìš© ë¡œì§ ì¶”ê°€ë¨
            raw_imp_score = self.model_imp.predict(input_list)[0]
            penalty = self.calculate_penalty(title) # ì œëª© ê¸°ë°˜ í˜ë„í‹° ê³„ì‚°
            
            final_imp_score = raw_imp_score - penalty
            importance = int(max(0, min(10, final_imp_score))) # 0~10 ë²”ìœ„ ì œí•œ

            # Sentiment (0.0~10.0)
            sent_score = self.model_sent.predict(input_list)[0]
            sentiment = round(max(0.0, min(10.0, sent_score)), 2)

            return {
                "main_category": str(main_cat),
                "sub_category": str(sub_cat),
                "importance": int(importance),
                "sentiment": float(sentiment),
                # "penalty_applied": penalty # ë””ë²„ê¹…ìš©: ì ìš©ëœ í˜ë„í‹° ì ìˆ˜ í™•ì¸
            }

        except Exception as e:
            print(f"âš ï¸ Prediction failed: {e}")
            return {
                "main_category": "ê¸°íƒ€",
                "sub_category": "ê¸°íƒ€",
                "importance": 5,
                "sentiment": 5.0
            }

# =========================================================
# 3. T5 í—¤ë“œë¼ì¸ ìƒì„±ê¸°
# =========================================================
class T5HeadlineGenerator:
    def __init__(self, model_dir=None):
        if model_dir is None:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            model_dir = os.path.join(current_dir, 'models', 't5-model')

        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        # symlinkì¸ ê²½ìš° ì‹¤ì œ ê²½ë¡œë¡œ ë³€í™˜ (transformersì˜ repo_id ê²€ì¦ ìš°íšŒ)
        model_dir = str(Path(model_dir).resolve())
        print(f"Loading T5 model from: {model_dir} (device: {self.device})")

        try:
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True, use_fast=True)
            except Exception:
                # transformers êµ¬ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ ì‹œ tokenizer.json ì§ì ‘ ë¡œë“œ
                from transformers import PreTrainedTokenizerFast
                tokenizer_file = os.path.join(model_dir, 'tokenizer.json')
                self.tokenizer = PreTrainedTokenizerFast(
                    tokenizer_file=tokenizer_file,
                    pad_token="<pad>",
                    eos_token="</s>",
                    unk_token="<pad>",
                )
            self.model = T5ForConditionalGeneration.from_pretrained(model_dir, local_files_only=True).to(self.device)
            self.model.eval()
            print("âœ… T5 model loaded successfully!")
        except Exception as e:
            print(f"âŒ Error loading T5 model: {e}")
            print(traceback.format_exc())
            self.model = None
            self.tokenizer = None

    def generate(self, text, max_new_tokens=64, num_beams=4):
        if self.model is None:
            return ""
        try:
            inputs = self.tokenizer(
                "summarize: " + text,
                return_tensors="pt",
                max_length=512,
                truncation=True
            ).to(self.device)

            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    num_beams=num_beams,
                    early_stopping=True
                )

            return self.tokenizer.decode(output_ids[0], skip_special_tokens=True)
        except Exception as e:
            print(f"âš ï¸ T5 generation failed: {e}")
            return ""

    def generate_batch(self, texts, max_new_tokens=64, num_beams=4):
        """ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        if self.model is None:
            return [""] * len(texts)

        prefixed = ["summarize: " + t for t in texts]
        try:
            inputs = self.tokenizer(
                prefixed,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            ).to(self.device)

            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    num_beams=num_beams,
                    early_stopping=True
                )

            results = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]
            print(f"ğŸ” T5 sample output (first 2): {results[:2]}")
            print(f"ğŸ” T5 raw token IDs (first item): {output_ids[0].tolist()[:20]}")
            return results
        except Exception as e:
            print(f"âš ï¸ T5 batch generation failed: {e}")
            print(traceback.format_exc())
            return [""] * len(texts)


# =========================================================
# 4. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
# =========================================================
if __name__ == "__main__":
    test_title = "ì¡¸ì—…ì´ë‹¤"
    test_desc = "ë“œë””ì–´ í•™êµë¥¼ ì¡¸ì—…í•˜ê²Œ ë˜ì–´ ê¸°ì˜ë‹¤."

    classifier = NewsClassifier()
    result = classifier.predict(test_title, test_desc)
    print("\n--- ë¶„ì„ ê²°ê³¼ ---")
    print(f"ì œëª©: {test_title}")
    print(f"ê²°ê³¼: {result}")

    t5_title = "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì˜ì—…ì´ìµ 10ì¡° ëŒíŒŒ"
    t5_desc = "ì‚¼ì„±ì „ìê°€ ì˜¬í•´ 3ë¶„ê¸° ì˜ì—…ì´ìµì´ 10ì¡°ì›ì„ ë„˜ì–´ì„°ë‹¤ê³  ë°œí‘œí–ˆë‹¤. ë°˜ë„ì²´ ë¶€ë¬¸ íšŒë³µê³¼ ìŠ¤ë§ˆíŠ¸í° íŒë§¤ í˜¸ì¡°ê°€ ì£¼ìš” ì›ì¸ìœ¼ë¡œ ë¶„ì„ëœë‹¤."
    t5 = T5HeadlineGenerator()
    headline = t5.generate(t5_title + " " + t5_desc)
    print("\n--- T5 í—¤ë“œë¼ì¸ ìƒì„± ê²°ê³¼ ---")
    print(f"ì…ë ¥: {t5_title} / {t5_desc}")
    print(f"ìƒì„±ëœ í—¤ë“œë¼ì¸: {headline}")