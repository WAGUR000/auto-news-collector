2025-09-09 09:07 빌드 = 10개 저장

2025-09-09 09:14 빌드 = 100개 저장

2025-09-09 10:04 오류 발생 = 기본키 값 누락 -> 가끔 JSON 파일에서 original link, description이 비어있는 경우가 있음

2025-09-09 11:30 = 100개저장, link를 파티션 키(기본 키)로 변경

2025-09-10 10:39 = 어제날짜의 뉴스정보를 읽어오는 파이썬 파일을 만들었음. 다만 아직 읽어오고 파이썬에서 print하도록 만들었을 뿐이므로, 어떻게 활용할지 생각해야 함

2025-09-10 14:50 = 해당 Actions를 트리거로 하는 lambda 함수를 작성해서 테스트 작업. Layer 설정 부족, boto 충돌 , Deserializer() 오류 해결 필요

2025-09-10 15:53 = 테스트 작업. Layer 설치시 Linux 기반으로 변경, TypeDeserializer() 으로 변경,  DynamoDB에 반영되지 않아서 확인

2025-09-10 16:37 = 테스트 작업. 환경변수 TABLE_NAME value값에 오입력된 공백 제거, 로그를 통한 API 회신 확인, Free Tier 호출 제한 발생

2025-09-10 16:51 = 테스트 작업. time.sleep(1)을 적용하여 분당 요청수 제한을 넘지 않도록 설정

2025-09-10 16:51 =  테스트 작업. import time 작성 -> 429 발생. 한개의 뉴스당 1개의 request를 요청하기 때문에, 지속적으로 확인해보기 위해 Commit을 한 결과, 일일 한도인 1000개에 도달하였다.

2025-09-11 10:45 = Gemini AI의 모델을 2.5 Flash로 변경하고, 90일 무료 플랜 (Tier 1)으로 변경하였다. 어제 얻은 정보를 토대로 토큰수와 Request 수를 줄이기 위해 한번의 요청당 10개의 기사를 보내고 10개의 기사를 받아오는
형태로 20분당 10번의 요청, 하루에 720번의 요청을 진행하기 때문에 무료 플랜으로 돌아가도 정상 작동할 것이다. 1개의 요청당 15초정도 소요되어, Git Hub Actions가 workflow를 실행시킬때 약 3분정도 소요된다. 오전 10:45이후로 저장되는 데이터에는 소분류, 대분류, 중요도, 감정, 토픽이 분류된다. 이를 통하여 하루에 한번, 일주일에 한번, 한달에 한번 분석/시각화를 하기 쉬워져서, 이를 Actions로 자동화하는 작업을 해도 유효할 것으로 보인다.

2025-09-11 13:00 = category1, category2같은 중복값, topics와 같은 잘못된 테이블값을 제거했다.

2025-09-12 오후 3:33 = 수집하고 있는 정보를 웹페이지에서 서비스를 제공하기 위해서는, 현재 DynamoDB(파티션키 = News)로 조회 기능을 사용하기에는 Query사용이 불가능하여 적합하지 않다. 새로운 DB(News_Data_v1)을 생성하여 PK,SK를 새롭게 지정하여 최신 정렬이 가능하도록 만들고, GSI를 이용하여 중요도순, 토픽순 정렬을 가능하도록 만들 예정이다.

2025-09-12 15:37 = 기존 News가 PK였던 DynamoDB 테이블의 경우 Query를 할 수 없어 데이터가 많아 질수록 조회가 어려워져서, PK(Date), SK(Date-HH-MM-ss-시간대#링크)인 News_Data_v1 테이블을 생성하였다.

2025-09-12 17:52 = main_category 분석에 대해 prompt로 '대분류를 해달라'라고 했을뿐 기준이 없어서 여러 대분류가 존재했으나, 정치,경제,사회,IT/과학,문화/생활,연예,스포츠,국제 중 하나로 정하도록 변경하였다.

2025-09-15 14:54 = 주말간 Actions의 스케쥴 작업에서 AI가 Importance를 string으로 주는 경우가 있어 GSI와 맞지 않아 데이터를 저장하지 못하는 경우가 존재하여, Int로 바꾸는 과정 추가. (+쓰지 않는 workflow 삭제)

2025-09-15 17:09 = 로컬테스트 환경과 테스트 workflow / 스케쥴 workflow를 분리하였다.

2025-09-16 14:02 = sentiment(감정)을 부정/중립/긍정에서 0.0(부정)~10.0(긍정)으로 세분화 하였고, link를 통해 매치되는 언론사가 있는지 확인하고 없으면 기타언론사로 분류하도록 변경했다.

2025-09-16 14:30 = 군집화 과정을 추가하여 중복 기사가 조회되지 않도록 만들고, DynamoDB에 float형인 sentiment가 들어갈 수 없어서 Decimal형식으로 변환하여 저장하도록 변경하였다.

2025-09-16 14:41 = cluster-id의 유일성을 보장하기위해 link를 이어붙였으나 지나친 데이터중복 / 불필요한 저장이라 생각하여 uuid를 생성하여 저장하도록 변경하였다.

2025-09-16 14:48 = 테스트 환경에서의 최신 DB에서 꺼내오는 batch수를 제한하였다.

2025-09-16 15:14 = torch가 포함됨에 따라 캐싱을 하더라도 의존성파일 설치에 시간이 더 걸리게 되어, 기존 10분 제한을 15분으로 변경하였다.

2025-09-16 16:28 = GSI에서 대표군집인지 확인하는 (0,1) 속성을 포함시키고 이진으로 했더니 계속해서 오류가 발생하여 GSI를 제거하고 이전형태로 되돌렸다.

2025-09-17 15:35 = 군집로직을 개선하고 DB에서 읽어오는 갯수를 100개에서 300개로 증가시켰으며, 모듈화를 진행하고 readme에 Commit했던 이력에 대한 부가설명을 작성하였다.


요약 : Python, GitHub Actions와 Amazion Free tier일때 사용하기 적합한 DynamoDB, 네이버 검색 API를 이용하여 수집한 뉴스정보를 Gemini 2.5 Flash API가 분석한 정보를 포함하고, 군집화를 한뒤 AWS DynamoDB에 저장

**GitHub Actions** - 자동화
해당 Repository의 workflows yml파일에 따라, 20분마다 py파일을 이용하여 100개의 뉴스기사를 수집한다.
Repository에서 미리 지정한 액세스 코드를 통하여, Amazon DynamoDB에 접근하여 데이터를 저장한다.
해당 Repository가 'Public'인 이상, 비용은 발생하지 않는다. 


**Amazon DynamoDB** - 데이터베이스 
프리티어 기준으로 25GB와 빠른 읽기/쓰기를 제공한다.


**IAM** - DB접근 권한
GitHub Actions가 DynamoDB 접근하기 위해 먼저 설정해야 하는 부분이다.
사용자 그룹(GitHub Worker로 지정함)을 생성하여 DynamoDB 접근권한만을 부여하고, CLI 액세스코드를 생성하여 GitHub Actions가 사용할 수 있도록 했다.


**네이버 API** - API
keyword를 뉴스, 갯수를 100개, date순(최신순)으로 하여 뉴스정보 100개를 수집한다.
API를 이용하기 때문에 response로 받는 JSON 파일 안의 정보외에는 획득 할 수 없지만, 기본적으로 대부분의 뉴스사이트, 네이버뉴스도 기본적으로 크롤링을 허용하지 않으므로, 이를 활용한다.

**Gemini API** - AI API
토픽, 대분류/소분류는 가능하겠지만 중요도, 감정은 꽤 심화적인 머신러닝이 필요할 것으로 보여 AI API를 이용하여 이 과정을 대체하였다. News에 대해 AI API는 대분류, 소분류, 토픽, 중요도(1-10), 감정판별을 진행하여 반환해준다.




생각할 점
- 보안 : GitHub Actions에겐 Amazon IAM에서 CLI 액세스 코드를 통하여 DynamoDB의 모든 권한을 부여하였는데, 조금 더 강한 보안을 위해서라면 다른 방법으로 권한을 부여하거나, DynamoDB에서 쓰기만 가능하도록 만드는 것도 좋아보인다. -> 지금은 Actions가 Scan이 가능해야하니, 이 부분은 그대로 두어도 될 것 같다. -> **현재는 군집화 과정에서 테이블의 최신값을 불러오므로 현재의 권한(DynamoDB Full Access)을 유지한다.**

- 확장 : 나중에 크롤링을 통해서도 정보를 수집하고, 검색어 트렌드 등과 결합할 수 있을수도 있는데, 이에 대해 생각해보면 좋을 것 같다. (DynamoDB는 RDB와 달리 NoSQL DB형식이라, 크롤링을 통해 원래정보 + 기타정보가 들어오면 그냥 새로운 속성을 같이 사용하여 같은 DynamoDB를 사용해도 문제는 없는 것 같다.) **(보류)**

- 빌드 테스트 : API를 이용한 자동수집은 마무리 되었으나, 이후 수정한다면 수정할때마다 자동으로 테스트하는 기능을 넣으면 좋을 것 같다. -> 다만, 이 Repository 안에서 수정이 일어날 경우(심지어 README.md를 수정할때도) 무조건적으로 main.yml이 실행되어 100개의 데이터를 받아오고 분석하고 저장하고 있는 형태이다. 워크플로우를 분리하여 변경시 테스트, 스케쥴 작업으로 나눠 두는것이 좋아 보인다. **(완료)**

- Airflow : Airflow를 실습한 뒤 Actions를 사용하였는데, 매우 단순한 작업이라면 Actions를 이용한 작업으로 충분하나, 복잡해지거나 순환과정 같은 부분이 생길경우 AirFlow가, DAG가 서로 얽혀 관리가 필요하다면 Kubernetes가 사용된다.

- AWS EventBridge - 만약 Git Hub Actions같은 역할을 AWS안에서 모두 해결하고 싶다면 AWS Eventbridge를 이용할 수 있다.


이전에 생각했던 것 (2025-09-10)
- 문제1. for 문을 사용하여 한 record당 1 request를 보내어, 하나의 결과물 (기본키, 주요 토픽, 대분류, 소분류, 중요도)을 받는 구조의 문제. 다만 여러개를 한번에 줄경우 답변에 대한 결과 분석이 어려워 질 수 있다. 다만, 일일 한도 1000개로는 턱없이 부족하므로, 해당 방법은 바꿔야 한다.
- 생각해본 점1. 굳이 INSERT 스트림을 감지하여 100개가 들어올때마다 분류를 할 필요가 있을까? 그런건 아니지만, 어제의 데이터 (펜듈럼 now에서 day-1해서 09-09)를 뽑았을때 1500개 정도의 로그만 남겨져 있던걸 보면 다 추출이 안되었을지도 모르니 이런 방법을 사용했던 것이지만, 무료 API 한도가 존재하여 이 방법은 좋은 방법이 아니다.
- 생각해본 점2. '로그'가 6000개중 일부만 받아온게 아닐까? 일단 lambda 함수의 코드를 모두 주석처리하고, 혹시 AI API를 사용할때 csv파일로 보내고, csv파일로 받을 수 있는지도 확인해보면 좋을것 같다. 
- 배운 점 : lambda는 Window bash에서 설치한 py 컨테이너와 충돌을 일으키는 부분이 있어, 레이어를 만들때 어느정도 확인을 해야하는 부분이 있다. 스트림을 활성화해서 INSERT 신호가 들어오면 트리거 되는 형식으로 작성되었는데, 이러한 기능은 꽤 도움이 될 것 같다. 당연하지만, lambda의 IAM 권한에서 DynamoDB에 대한 접근권한을 허용해야만 가능하다.
- 개선점 : 클라우드 서비스를 이용해보고 싶어서 lambda를 사용했지만, 현재 환경에서 lambda를 사용할거라면 AWS AI나 AWS 머신러닝쪽으로 보내는 것이 현재 방법보다 좋지만 비용이 발생한다. 따라서 GitHub Actions에서 Naver API를 이용하여 JSON 파일을 받아오고, 이를 AI API에게 보내 분류를 시키는것이 더 좋은 개선 방법인 것 같다. 로컬 LLM은 컴퓨터가 켜져있어야 하니 자동화에는 어울리지 않고, 머신러닝을 직접 짜는것은 DT쪽에 더 가깝고, 분류도 정확하지 않을 수 있다.

